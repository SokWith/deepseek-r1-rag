import streamlit as st
from langchain_community.llms import Ollama
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Deepseek ì±—ë´‡",
    page_icon="ğŸ¤–",
)

# ê¸°ë³¸ ìŠ¤íƒ€ì¼ ì„¤ì •
st.markdown("""
    <style>
    .stApp {
        max-width: 1200px;
        margin: 0 auto;
    }
    .model-name {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
        text-align: center;
        font-weight: bold;
    }
    </style>
""", unsafe_allow_html=True)

# ëª¨ë¸ ì •ë³´ í‘œì‹œ
st.markdown('<div class="model-name">ğŸ¤– Model: Deepseek-r1:32b</div>', unsafe_allow_html=True)

# ì œëª©
st.title("ğŸ¤– Deepseek ì±—ë´‡")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì„¤ì •
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œë§Œ ëŒ€ë‹µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ì–´ë–¤ ì§ˆë¬¸ì´ ë“¤ì–´ì™€ë„ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
ì˜ì–´ë‚˜ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ëœ ì§ˆë¬¸ì´ ë“¤ì–´ì™€ë„ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
ì „ë¬¸ì ì¸ ë‚´ìš©ë„ ëª¨ë‘ í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.

ê·œì¹™:
1. í•­ìƒ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•  ê²ƒ
2. ì „ë¬¸ ìš©ì–´ë„ ê°€ëŠ¥í•œ í•œêµ­ì–´ë¡œ ì„¤ëª…í•  ê²ƒ
3. ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•  ê²ƒ
4. ê³µì†í•˜ê³  ì •ì¤‘í•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•  ê²ƒ

ì´ì œ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤."""

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("ì„¤ì •")
    temperature = st.slider("Temperature", min_value=0.0, max_value=1.0, value=0.7, step=0.1)
    
    if st.button("ëŒ€í™” ë‚´ì—­ ì§€ìš°ê¸°"):
        st.session_state.messages = []
        st.rerun()
    
    st.markdown("---")
    st.markdown("### ì‚¬ìš© ë°©ë²•")
    st.markdown("""
    1. ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”
    2. Temperatureë¡œ ì°½ì˜ì„±ì„ ì¡°ì ˆí•˜ì„¸ìš”
    3. 'ëŒ€í™” ë‚´ì—­ ì§€ìš°ê¸°'ë¡œ ìƒˆë¡œìš´ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”
    """)

def get_llm():
    try:
        return Ollama(
            model="deepseek-r1:32b",
            temperature=temperature,
            callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
            additional_kwargs={
                "gpu": True,  # í•­ìƒ GPU ì‚¬ìš©
                "threads": 8,
                "mmap": True
            }
        )
    except Exception as e:
        st.error(f"Ollama ì—°ê²° ì˜¤ë¥˜: {str(e)}")
        st.info("Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None
    
    
# ë©”ì‹œì§€ ì´ë ¥ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# ì±„íŒ… ì…ë ¥ ë° ì‘ë‹µ
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    # AI ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        try:
            llm = get_llm()
            if llm:
                message_placeholder = st.empty()
                with st.spinner("ì‘ë‹µ ìƒì„± ì¤‘..."):
                    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ê²°í•©
                    full_prompt = f"{SYSTEM_PROMPT}\n\nì‚¬ìš©ì: {prompt}\n\nì‘ë‹µ:"
                    
                    response = llm(full_prompt)
                    message_placeholder.write(response)
                    st.session_state.messages.append({"role": "assistant", "content": response})
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            st.info("""
            ë‹¤ìŒ ì‚¬í•­ë“¤ì„ í™•ì¸í•´ì£¼ì„¸ìš”:
            1. Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ê°€ìš”?
            2. Deepseek ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆë‚˜ìš”? (`ollama pull deepseek` ì‹¤í–‰)
            3. ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ì •ìƒì¸ê°€ìš”?
            """)