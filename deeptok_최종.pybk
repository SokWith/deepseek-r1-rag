import streamlit as st
from langchain_community.llms import Ollama
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# é¡µé¢è®¾ç½®
st.set_page_config(
    page_title="Deepseek èŠå¤©æœºå™¨äºº",
    page_icon="ğŸ¤–",
)

# åŸºæœ¬æ ·å¼è®¾ç½®
st.markdown("""
    <style>
    .stApp {
        max-width: 1200px;
        margin: 0 auto;
    }
    .model-name {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
        text-align: center;
        font-weight: bold;
    }
    </style>
""", unsafe_allow_html=True)

# æ¨¡å‹ä¿¡æ¯æ˜¾ç¤º
st.markdown('<div class="model-name">ğŸ¤– æ¨¡å‹: Deepseek-r1:32b</div>', unsafe_allow_html=True)

# æ ‡é¢˜
st.title("ğŸ¤– Deepseek èŠå¤©æœºå™¨äºº")

# ä¼šè¯çŠ¶æ€åˆå§‹åŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []

# åŸºæœ¬æç¤ºè®¾ç½®
SYSTEM_PROMPT = """æ‚¨æ˜¯ä¸€ä¸ªåªç”¨ä¸­æ–‡å›ç­”é—®é¢˜çš„ AI åŠ©æ‰‹ã€‚
æ— è®ºæ”¶åˆ°ä»€ä¹ˆè¯­è¨€çš„é—®é¢˜ï¼Œæ‚¨éƒ½å¿…é¡»ç”¨ä¸­æ–‡å›ç­”ã€‚
å³ä½¿æ˜¯ä¸“ä¸šå†…å®¹ï¼Œä¹Ÿè¯·ç”¨ä¸­æ–‡è§£é‡Šã€‚

è§„åˆ™ï¼š
1. å§‹ç»ˆç”¨ä¸­æ–‡å›ç­”
2. å°½å¯èƒ½ç”¨ä¸­æ–‡è§£é‡Šä¸“ä¸šæœ¯è¯­
3. è§£é‡Šè¦æ¸…æ™°æ˜“æ‡‚
4. ä½¿ç”¨ç¤¼è²Œã€æ­æ•¬çš„è¯­æ°”

ç°åœ¨å¼€å§‹å¯¹è¯ã€‚"""

# ä¾§è¾¹æ è®¾ç½®
with st.sidebar:
    st.header("è®¾ç½®")
    temperature = st.slider("æ¸©åº¦", min_value=0.0, max_value=1.0, value=0.7, step=0.1)
    
    if st.button("æ¸…é™¤å¯¹è¯è®°å½•"):
        st.session_state.messages = []
        st.rerun()
    
    st.markdown("---")
    st.markdown("### ä½¿ç”¨æ–¹æ³•")
    st.markdown("""
    1. è¾“å…¥æ¶ˆæ¯
    2. ä½¿ç”¨æ¸©åº¦è°ƒèŠ‚åˆ›é€ åŠ›
    3. ç‚¹å‡»â€œæ¸…é™¤å¯¹è¯è®°å½•â€å¼€å§‹æ–°å¯¹è¯
    """)

def get_llm():
    try:
        return Ollama(
            model="deepseek-r1:32b",
            temperature=temperature,
            callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
        )
    except Exception as e:
        st.error(f"Ollama è¿æ¥é”™è¯¯: {str(e)}")
        st.info("è¯·ç¡®ä¿ Ollama å·²å¯åŠ¨ã€‚")
        return None
    
# æ˜¾ç¤ºæ¶ˆæ¯å†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# èŠå¤©è¾“å…¥åŠå›å¤
if prompt := st.chat_input("è¯·è¾“å…¥æ¶ˆæ¯..."):
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    # ç”Ÿæˆ AI å›å¤
    with st.chat_message("assistant"):
        try:
            llm = get_llm()
            if llm:
                message_placeholder = st.empty()
                with st.spinner("æ­£åœ¨ç”Ÿæˆå›å¤..."):
                    # ç»“åˆç³»ç»Ÿæç¤ºå’Œç”¨æˆ·æç¤º
                    full_prompt = f"{SYSTEM_PROMPT}\n\nç”¨æˆ·: {prompt}\n\nå›å¤:"
                    
                    response = llm(full_prompt)
                    message_placeholder.write(response)
                    st.session_state.messages.append({"role": "assistant", "content": response})
        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
            st.info("""
            è¯·æ£€æŸ¥ä»¥ä¸‹äº‹é¡¹ï¼š
            1. Ollama æ˜¯å¦å·²å¯åŠ¨ï¼Ÿ
            2. æ˜¯å¦å·²å®‰è£… Deepseek æ¨¡å‹ï¼Ÿï¼ˆè¿è¡Œ `ollama pull deepseek`ï¼‰
            3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸ï¼Ÿ
            """)
